# Lab 2 - Load data from the dockerized MySQL into RDS 

The objective of this lab is for you to also get experience with extracting data from a SQL database. 
We are going to use the dockerized MySQL db with NBA data created in the previous lab and we are 
going to extract the data and load it into a database hosted in AWS. 

The following diagram can give you a visual overview of how are we going to connect our services together. 
![Lab1 Architecture Diagram](documentation_images/Lab2_MySQL_Load_into_RDS.png)

**Pre-requisites:** 
* Lab 1 completed 
* AWS Enroute Account
* Clone this repository
* https://www.enterprisedb.com/downloads/postgres-postgresql-downloads
* Download [psql](https://www.postgresql.org/download/)

## 1. Throwback 
You will need to run the dockerized MySQL database and load the NBA as per the lab 1 requested. 
```
docker-compose up -d
```

## 2. AWS

### 2.1 AWS configure
 TBD: maybe SSO it's ready for this but basically configure with aws configure command
 
### 2.2 AWS RDS 
AWS RDS (Relational Database Service) is a managed SQL database service, it supports multiple database engines to store and
organize data, it also helps with relational database management tasks, such as data migration, backup, recovery and patching.

The RDS instance that we are going to use in this lab is already up and running, so you will only need to log into the db, 
create a database, create the tables and start loading your data. 


The RDS instance is using IAM database authentication, which is more secure than native authentication methods because of the following:
* IAM generates db authentication tokens using your AWS access keys (you don't need to store database user creds)
* Auth tokens have a lifespan of 14 minutes
* IAM db auth requires an SSL connection, therefore everything that you transmit to and from your RDS DB instance is encrypted 

![IAM Database Authentication](documentation_images/IAM_dabaseAuth.png)

Follow the steps: 
1. Generate an AWS authentication token to identify the IAM role by running `generate-db-auth-token` with the aws username you were assigned.
```shell script
export RDSHOST="aurorapg-ssl.cluster-XXXXXXXXXXX.us-west-2.rds.amazonaws.com"
export PGPASSWORD="$(aws rds generate-db-auth-token --hostname $RDSHOST --port 5432 --region us-west-2 --username iamuser)"
```
2. This will create a temporary password
```shell script
echo $PGPASSWORD
```

3. Connect to the AWS RDS DB instance using IAM role credentials and the signed IAM authentication token through a client (pgAdmin) or through the cli: 

**Option 1: Through pgAdmin ** 

Launch the pgAdmin application and add a new server: 
In **General** tab, enter a name to identify the server, and deselect **Connect now?**:
![pgAdmin](documentation_images/pgAdmin_connect1.png)

In **Connection** tab, enter the aws db host, the assigned port and the username (corresponding to your IAM user):
![pgAdmin](documentation_images/pgAdmin_connect2.png)

Under **SSL** tab, change to **Require** and click **Save**:
![pgAdmin](documentation_images/pgAdmin_connect3.png)

Go to the server in the left panel, right click and click **Connect Server**:
![pgAdmin](documentation_images/pgAdmin_connect4.png)

Enter the temporary password generated by the `generate-db-auth-token` command and click `OK`: 
![pgAdmin](documentation_images/pgAdmin_connect5.png)

**Option 2: Through cli ** 
We are going to connect to a default database called `postgres`
```
psql "host=$RDSHOST port=5432 sslmode=require   user=dpatron dbname=postgres password=$PGPASSWORD"
```

### 2.3 Creating the objects

Now that you are connected, you have to create a database to hold your tables, please create a database with the following 
pattern: `<iamuser>_nba_data`

### ✏️  Exercise 1 
Generate the DDL's to create the tables under your database

## 3. Python
From the last lab we know how to read data from an API and load it into a SQL database, now we are going to read from a SQL database
and insert into another SQL db. 

### 3.1 The requirements.txt
We are going to use: 
* json - to transform text into json
* configparser - for configuration management
* pandas - to load our json into a DataFrame 
* sqlalchemy - to make the connection between Python and the dockerized MySQL
* boto3: to connect to AWS RDS 

Create a new virtual environment, activate it and install the requirements: 
```shell script
python3 -m venv /path/to/new/virtual/environment
source /newvirtualenvironment/bin/activate
pip install -r requirements.txt
```

### 3.1 IAM authentication
Just like we did through the console, we neeed to generate a signed IAM authentication token, and for
that we are going to leverage the credentials stored under  `aws/credentials`, make sure you select the 
correct profile if you have multiple profiles set and we are going to create an `rds` client. 

```python
session = boto3.Session(profile_name='default')
client = session.client('rds')
```

Then, we are going to execure the `generate-auth-token` but with the boto3 Python SDK:
 ```python
token = client.generate_db_auth_token(DBHostname=pg_db_host, Port=pg_db_port, DBUsername=pg_db_user, Region=pg_db_region)
```

Now that we have our temporary password, we can now create a new connectiong string to connect to our psotgres database: 
    
````python
pg_engine = create_engine(f"postgresql+psycopg2://{pg_db_user}:{token}@{pg_db_host}:{pg_db_port}/{pg_db_name}")

````    

### ✏️  Exercise
Load all the tables created in the lab 1 into the PostgreSQL db: 
* Teams
* Games
* Players 




PANADS https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.read_sql.html